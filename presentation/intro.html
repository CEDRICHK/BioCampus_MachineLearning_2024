<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Cédric Hassen-Khodja, Volker Baeker, Clément Benedetti, Thibault Odor" />
    <script src="intro_files/header-attrs/header-attrs.js"></script>
    <script src="intro_files/xaringanExtra-progressBar/progress-bar.js"></script>
    <script src="intro_files/htmlwidgets/htmlwidgets.js"></script>
    <script src="intro_files/d3/d3.min.js"></script>
    <script src="intro_files/dagre/dagre-d3.min.js"></script>
    <link href="intro_files/mermaid/dist/mermaid.css" rel="stylesheet" />
    <script src="intro_files/mermaid/dist/mermaid.slim.min.js"></script>
    <link href="intro_files/DiagrammeR-styles/styles.css" rel="stylesheet" />
    <script src="intro_files/chromatography/chromatography.js"></script>
    <script src="intro_files/DiagrammeR-binding/DiagrammeR.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="custom-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Introduction to Machine Learning
]
.author[
### Cédric Hassen-Khodja, Volker Baeker, Clément Benedetti, Thibault Odor
]

---






<style>.xe__progress-bar__container {
  top:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 10px;
  background-color: #0051BA;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

## Outlines

- Introduction to Machine Learning
  - Definitions and Key Concepts
  
  - Types of Machine Learning

- Approaches and Applications
  - Data-Driven Approach
  
  - Image Classification
  
  - Classification Models and Algorithms
  

- Software Overview
  - CellProfiler &amp; CellProfiler Analyst
  
  - QuPath
  
---

## Machine Learning - Definitions

- **Machine Learning** is concerned with the technology that enables computer 
programs to improve their performance at a certain task by experience.

--

- In Machine Learning we want to infer (learn) some function `f` from data, 
capable of predicting the output `y` from an input (measurement) x:

`$$y = f(x)$$`

--

- The data is used to learn `f` is called **training set**.

--

- In this general formulation, there is no particular limitation as to the 
mathematical nature of `x` and `y` . In many cases `x` is a P-dimensional vector
and `y` a categorical or continuous output variable, but there are 
other settings, where `x` and / or `y` are more complicated objects, 
such as images or graphs.

---

## Types of Machine Learning

.pull-left[
<div class="DiagrammeR html-widget html-fill-item" id="htmlwidget-1d05e766c55abad844ac" style="width:120%;height:252px;"></div>
<script type="application/json" data-for="htmlwidget-1d05e766c55abad844ac">{"x":{"diagram":"\n  graph TD\n  A{<b>Machine Learning<\/b>} --> B{<b>Supervised Learning<\/b>}\n  A --> C(<b>Unsupervised Learning<\/b>)\n  A --> D(<b>Reinforcement Learning<\/b>)\n  B --> E[<b>Classification<\/b>]\n  B --> F[<b>Regression<\/b>]\n  style A fill:#E5E25F\n  style B fill:#E5E25F\n  style C fill:#87AB51\n  style D fill:#87AB51\n  style E fill:#B6E6E6\n  style F fill:#B6E6E6\n"},"evals":[],"jsHooks":[]}</script>
]

.pull-right[
- In **supervised learning**, the training data contains both measurements `\(x_i\)`
and the corresponding output variables `\(y_i\)`.   
Together, they build the training set `T`:
`$$T = {(x_i, y_i)}i = 1,...,N$$`
- In **unsupervised learning**, there are no annotations `\(y_i\)`. We aim at inferring
**patterns** from the data.
]

---

## Classification vs Clustering

&lt;img src="./figures/classification_clustering_2.png" width="60%" /&gt;
- In **supervised learning**, we start from annotated data (here
annotation is illustrated by the color), and we wish to learn a
decision boundary that allows us to tell these classes apart.

- In **unsupervised learning**, we start with a point clouds and
we wish to identify classes directly from their distribution.

---

## Machine Learning: Data Driven Approach

&lt;img src="./figures/supervised_ML.png" width="62%" /&gt;

1. Collect a dataset of images and labels.
2. The dataset will be splitted into **Training set** and **Test set**.
3. Use Machine Learning algorithm to train a **classifier** using the training dataset.
4. The classifier will be used to **predict** the labels of the images from the test dataset.
5. The predicted labels will be subsequently compared with the **ground-truth** labels to evaluate the performance of the classifiers.
6. Evaluate the classifier on **new images**.

---

## Image classification

We are going to use a classic dataset in machine learning, https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits, consisting of 8x8 grayscale images of handwritten digits.


### Loading the MNIST (handwritten digits) dataset



```python
# Import the digits dataset
from sklearn import datasets

# Load the digits dataset
digits = datasets.load_digits()
```

---

### Visualizing the dataset

.pull-left[

```python
import numpy as np
import matplotlib.pyplot as plt

# Set the figure size
plt.figure(figsize=(5, 5))

# Display the first 16 images
for index, (image, label) in enumerate(zip(digits.data[:16], digits.target[:16])):
    plt.subplot(4, 4, index + 1)
    plt.imshow(np.reshape(image, (8, 8)), cmap=plt.cm.gray)
    plt.title('%i' % label, fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()
```
]
.pull-right[
&lt;img src="intro_files/figure-html/unnamed-chunk-7-1.png" width="100%" /&gt;
]

---

### Splitting the dataset 


```python
from sklearn.model_selection import train_test_split

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(digits.data, 
digits.target, test_size=0.2, random_state=42)

# Display a summary of the data sets
print(f"Size of training data: {X_train.shape[0]}")
```

```
## Size of training data: 1437
```

```python
print(f"Size of test data: {X_test.shape[0]}")
```

```
## Size of test data: 360
```

---

### SVM classifier

Support Vector Machine is a supervised learning algorithm which &lt;u&gt; aims to separate the classes through a straight line&lt;/u&gt;, by maximizing the margins between the classes.

#### Hard Margin

.pull-left[
![](./figures/illustrated_svm.png)
]

.pull-right[
At inference time, what lies on the right side of the margin will be classified as "class 1" and vice versa.   
   
**Constraints..**
$$
`\begin{aligned}
y_i &amp;= +1 \mapsto \vec{w}\vec{x_i}+b \ge 1 \\
y_i &amp;= -1 \mapsto \vec{w}\vec{x_i}+b \le -1
\end{aligned}`
$$

]

---

### SVM classifier

#### Soft Margin

.pull-left[
![](./figures/illustrated_svm2.png)
]

.pull-right[

$$
`\begin{aligned}
C\underset{i}\sum{\varepsilon_i}\;s.t.\;y_i(\vec{w}\vec{x_i}+b) \ge 1 - \varepsilon_i\\ 
\forall i=1,...,n  
\end{aligned}`
$$
- if C is small, the margin is big
- If C `\(\mapsto \infty\)`, hard svm 
- If `\(0 &lt; \epsilon \le 1\)` (correct)
- If `\(\epsilon &gt; 1\)` (wrong)
]

---

### SVM classifier

#### Kernel trick

.pull-left[
&lt;img src="./figures/kernel-2.png" width="100%" /&gt;
]

.pull-right[
**Common Kernels**

- Sigmoid kernel: `\(K(x_i,x_j) = tanh(\alpha x_i^Tx_j+c)\)`

- Polynomial kernel: `\(K(x_i, x_j) = (x_i.x_j+1)^p\)`

- Radial Basis Function: `\(K(x_i, x_j) = exp(-\gamma(x_i - x_j)^2)\)`

]

---

### Train, predict the SVM classifier using the MNIST dataset


```python
from sklearn.svm import SVC

# Create an SVM classifier
svm = SVC(kernel='linear')

# Train the classifier
svm.fit(X_train, y_train);

# Predict using the classifier
y_pred = svm.predict(X_test)
```
&lt;img src="intro_files/figure-html/unnamed-chunk-11-1.png" width="100%" /&gt;
---

### Evaluate the SVM classifier

#### Confusion Matrix

| |Predicted "non-spam"| Predicted "spam"|
--|--------------------|-----------------|
|Actual "non-spam"| 50 | 10|
|Actual "spam"| 5 | 35 |

- **True Positives (TP)**: Emails that are actually spam and the algorithm correctly classified as such. In our example, there are 35 TPs.

- **True Negatives (TN)**: Emails that aren't spam and the algorithm correctly classified as non-spam. In our example, there are 50 TNs.

- **False Positives (FP)**: Emails that aren't spam but the algorithm incorrectly classified as spam. In our example, there are 10 FPs.

- **False Negatives (FN)**: Emails that are actually spam but the algorithm incorrectly classified as non-spam. In our example, there are 5 FNs.

&lt;u&gt;Confusion matrix&lt;/u&gt; is useful to calculate precision, recall, and accuracy!!

---

### Evaluate the SVM classifier (continued)

#### Precision



`$$Precision = \frac{TP}{TP + FP}$$`
Out of all items predicted as positive by the model, how many were actually positive?

#### Recall

`$$Recall = \frac{TP}{TP + FN}$$`
Out of all items that are actually positive, how many did the model correctly predict as positive?

---

### Evaluate the SVM classifier (continued)

#### F1-Score

It's defined as the harmonic mean of precision and recall:

`$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$`

#### Accuracy

$$ Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$
Out of all predictions made by the model, how many were correct?

---

### Evaluate the SVM classifier using the MNIST dataset


```python
from sklearn import metrics
import pandas as pd
import seaborn as sn

# Confusion matrix
cm = metrics.confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, range(10), range(10))
sn.set(font_scale=1.2)
plt.figure(figsize=(14, 7))
sn.heatmap(df_cm, annot=True, annot_kws={"size":16}, fmt="g")
plt.show()
```

&lt;img src="intro_files/figure-html/unnamed-chunk-12-3.png" width="45%" /&gt;

---

### Evaluate the SVM classifier using the MNIST dataset (continued)


```python
# Print the classification report
print(
    f"Classification report for classifier {svm}:\n"
    f"{metrics.classification_report(y_test, y_pred)}\n"
)
```

```
## Classification report for classifier SVC(kernel='linear'):
##               precision    recall  f1-score   support
## 
##            0       1.00      1.00      1.00        33
##            1       0.97      1.00      0.98        28
##            2       1.00      1.00      1.00        33
##            3       0.97      0.94      0.96        34
##            4       0.98      0.98      0.98        46
##            5       0.96      1.00      0.98        47
##            6       1.00      1.00      1.00        35
##            7       0.97      0.97      0.97        34
##            8       1.00      0.97      0.98        30
##            9       0.95      0.93      0.94        40
## 
##     accuracy                           0.98       360
##    macro avg       0.98      0.98      0.98       360
## weighted avg       0.98      0.98      0.98       360
```

---

### Tuning the hyper-parameters of an classifier

#### GridSearchCV

- It helps to loop through predefined &lt;u&gt;hyperparameters&lt;/u&gt; and fit your estimator (model) on your training set.

- You can select the best parameters from the listed hyperparameters.

---

### Tuning the hyper-parameters of the SVM classifier using the MNIST dataset


```python
from sklearn.model_selection import GridSearchCV

# Defining the hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.0001, 0.001, 0.1, 1],
    'kernel': ['linear', 'rbf', 'poly']
}

# Initializing the SVM classifier
svc = SVC()

# Setting up GridSearchCV
model = GridSearchCV(svc, param_grid, n_jobs=10)

# Fitting the model with the training data
model.fit(X_train, y_train);

# After the grid search, you can get the best parameters like this:
print("Best parameters found:", model.best_params_)
```

```
## Best parameters found: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}
```

---

### Predict and Evaluate the "Best Model"


```python
# Predict using the best model
y_pred = model.best_estimator_.predict(X_test)

# Compute the confusion matrix
cm = metrics.confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using seaborn
df_cm = pd.DataFrame(cm, range(10), range(10))
sn.set(font_scale=1.2)
plt.figure(figsize=(14, 7))
sn.heatmap(df_cm, annot=True, annot_kws={"size":16}, fmt="g")
plt.show()
```

&lt;img src="intro_files/figure-html/unnamed-chunk-15-5.png" width="40%" /&gt;

---
### Evaluate the best classifier


```python
# Print the classification report
print(
    f"Classification report for classifier {model.best_estimator_}:\n"
    f"{metrics.classification_report(y_test, y_pred)}\n"
)
```

```
## Classification report for classifier SVC(C=10, gamma=0.001):
##               precision    recall  f1-score   support
## 
##            0       1.00      1.00      1.00        33
##            1       1.00      1.00      1.00        28
##            2       1.00      1.00      1.00        33
##            3       1.00      0.97      0.99        34
##            4       1.00      1.00      1.00        46
##            5       0.98      0.98      0.98        47
##            6       0.97      1.00      0.99        35
##            7       0.97      0.97      0.97        34
##            8       1.00      1.00      1.00        30
##            9       0.97      0.97      0.97        40
## 
##     accuracy                           0.99       360
##    macro avg       0.99      0.99      0.99       360
## weighted avg       0.99      0.99      0.99       360
```

---

### From SVM to Random Forest: Addressing Weaknesses

#### Weaknesses of SVM

- **Sensitivity to High Dimensionality**: SVMs can become less effective as the dimensionality and size of the dataset increase.
- **Kernel Selection and Tuning**: Choosing and tuning the appropriate kernel can be complex and non-intuitive.
- **Less Efficient on Large Data**: SVMs require longer computation time for large datasets.
- **Difficulty in Interpretation**: The decision boundaries in SVMs, especially with non-linear kernels, are often hard to interpret.

--

#### Advantages of Random Forest

- **Effective Handling of High Dimensionality**: Performs well even with a large number of features and data.
- **Robustness to Noisy Data**: Tolerates noise and outliers well.
- **Interpretability**: Provides more intuitive understanding through the aggregation of individual tree decisions.
- **Flexibility and Versatility**: Effective for a wide range of classification and regression tasks, with less risk of overfitting.

---

### Decision tree - Algorithm

.pull-left[
&lt;img src="./figures/dt_space.png" width="100%" /&gt;
]

.pull-right[
##### The (binary) decision tree
&lt;img src="./figures/decision_line.png" width="90%" /&gt;
]

---

### Decision tree - Node splitting

- **Gini impurity**: Select the one having lowest Gini Impurity   
$$
`\begin{aligned}
GiniImpurity &amp;= 1 - Gini \\
Gini &amp;= \sum_i(p_i^2)
\end{aligned}`
$$
- **Information Gain**: Select the one having highest IG or lowest Entropy   
$$
`\begin{aligned}
IG &amp;= 1 - Entropy \\
Entropy &amp;= -\sum_i(p_ilog_2p_i)
\end{aligned}`
$$
---
#### Example

&lt;img src="./figures/RF_algo.png" width="40%" /&gt;

&lt;img src="./figures/RF_gini.png" width="50%" /&gt;



---
#### Pros and cons of decision trees 

- Advantages

  - Simple to interpret.
  - Can handle numerical and categorical data.
  - Little data preparation.
  - Fast.
  
- Disadvanages

  - Susceptible to noise.
  - Greedy algorithm is locally optimal.
  - Chance of overfitting.

---

### Random Forest

- Random forest is a **collection of decision trees**.

- Each tree is trained on a **bootstrap sample** of the training data (random sampling with replacement)

- Each node considers a **random subset** of dimensions.

- As a consequence, each DT is different.

- At runtine:

  - The new observation is **classified by all N trees**.
  
  - **Majority vote**.
  
  - Additionally provides **uncertainty** information.

---

#### Classification using random forest

&lt;img src="./figures/RF.png" width="75%" /&gt;

---

#### Apply the Random Forest classifier on MNIST dataset



```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load MNIST dataset
digits = datasets.load_digits()

# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.5, random_state=42)

# Initialize the classifier
clf = RandomForestClassifier(n_estimators=100)

# Train the classifier
clf.fit(X_train, y_train);

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Random Forest Classifier: {accuracy:.4f}")
```

```
## Accuracy of Random Forest Classifier: 0.9611
```

---

## Introduction to CellProfiler 

### Open-source software for image analysis

.pull-left[

&lt;img src="./figures/favicon.png" width="15%" /&gt;

- Free and open-source; Windows, Mac, Linux.

- Image analysis &amp; **quantification**.

- Cited in **1000+** papers per year.

]

.pull-right[
&lt;img src="./figures/CP_Intro.png" width="80%" /&gt;
]

---

### Main CellProfiler window

&lt;img src="./figures/CP_window.png" width="80%" /&gt;

---

## Introduction to CellProfiler Analyst

### Open-source software for data exploration

&lt;img src="./figures/cpa.png" width="8%" /&gt;

- Free and open-source; Windows, Mac, Linux.

- Image-centric data analysis &amp; machine learning.

--

### Main CellProfiler Analyst window

&lt;img src="./figures/CPA_window.png" width="80%" /&gt;

---

## CellProfiler &amp; CellProfiler Analyst

### Demo: Using machine learning to perform translocation assay

Hypothesis: treatment causes GFP translocation from cytoplasm to nucleus.

&lt;img src="./figures/translocation.png" width="80%" /&gt;
---

## CellProfiler &amp; CellProfiler Analyst

### Exercise: Using machine learning to perform cell painting assay

&lt;img src="./figures/cell-painting.png" width="80%" /&gt;

---

## Introduction to QuPath

.pull-left[

&lt;img src="./figures/qupath-logo.png" width="15%" /&gt;

- Freely available and open-source; Windows, Mac, Linux.

- Dedicated to histopathology.

- Integration of machine learning.

- Growing academic recognition.

]

.pull-right[
&lt;img src="./figures/qupath_intro.png" width="100%" /&gt;
]

---

### Main QuPath window

&lt;img src="./figures/qupath_modules.png" width="80%" /&gt;

---

## QuPath

### Exercise: Lung Ki67 Cell Classification

The focus is on classifying cells based on Ki67 expression, which is a marker of cell proliferation.

&lt;img src="./figures/Lung-KI67.png" width="35%" /&gt;

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
